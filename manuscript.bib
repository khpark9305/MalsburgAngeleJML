@article{ArsendorpfEtAl2013,
  author = {Asendorpf, Jens B. and Conner, Mark and De Fruyt, Filip and De Houwer, Jan and Denissen, Jaap J. A. and Fiedler, Klaus and Fiedler, Susann and Funder, David C. and Kliegl, Reinhold and Nosek, Brian A. and Perugini, Marco and Roberts, Brent W. and Schmitt, Manfred and van Aken, Marcel A. G. and Weber, Hannelore and Wicherts, Jelte M.},
  title = {Recommendations for Increasing Replicability in Psychology},
  journal = {European Journal of Personality},
  volume = {27},
  number = {2},
  issn = {1099-0984},
  pages = {108--119},
  keywords = {replicability, confirmation bias, publication bias, generalizability, research transparency},
  year = {2013},
  abstract = {Replicability of findings is at the heart of any empirical science. The aim of this article is to move the current replicability debate in psychology towards concrete recommendations for improvement. We focus on research practices but also offer guidelines for reviewers, editors, journal management, teachers, granting institutions, and university promotion committees, highlighting some of the emerging and existing practical solutions that can facilitate implementation of these recommendations. The challenges for improving replicability in psychological science are systemic. Improvement can occur only if changes are made at many levels of practice, evaluation, and reward. Copyright © 2013 John Wiley & Sons, Ltd.},
}

@article{AngeleEtAl2013,
  author = {Angele, Bernhard and Tran, Randy and Rayner, Keith},
  title = {Parafoveal-foveal overlap can facilitate ongoing word identification during reading: {Evidence} from eye movements},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  year = {2013},
  publisher = {American Psychological Association},
  address = {US},
  volume = {39},
  number = {2},
  pages = {526--538},
  keywords = {Eye Movements, Foveal Vision, Reading, Visual Perception},
  abstract = {Readers continuously receive parafoveal information about the upcoming word in addition to the foveal information about the currently fixated word. Previous research (Inhoff, Radach, Starr, \& Greenberg, 2000) showed that the presence of a parafoveal word that was similar to the foveal word facilitated processing of the foveal word. We used the gaze-contingent boundary paradigm (Rayner, 1975) to manipulate the parafoveal information that subjects received before or while fixating a target word (e.g., news) within a sentence. Specifically, a reader's parafovea could contain a repetition of the target (news), a correct preview of the posttarget word (once), an unrelated word (warm), random letters (cxmr), a nonword neighbor of the target (niws), a semantically related word (tale), or a nonword neighbor of that word (tule). Target fixation times were significantly lower in the parafoveal repetition condition than in all other conditions, suggesting that foveal processing can be facilitated by parafoveal repetition. We present a simple model framework that can account for these effects. (PsycINFO Database Record (c) 2013 APA, all rights reserved)},
  issn = {1939-1277(Electronic);0096-1523(Print)},
}

@article{BarrEtAl2013,
  title = {Random effects structure for confirmatory hypothesis testing: {Keep} it maximal},
  journal = {Journal of Memory and Language},
  volume = {68},
  number = {3},
  pages = {255--278},
  year = {2013},
  issn = {0749-596X},
  author = {Dale J. Barr and Roger Levy and Christoph Scheepers and Harry J. Tily},
  keywords = {Linear mixed-effects models, Generalization, Statistics, Monte Carlo simulation},
  abstract = {Linear mixed-effects models (LMEMs) have become increasingly prominent in psycholinguistics and related areas. However, many researchers do not seem to appreciate how random effects structures affect the generalizability of an analysis. Here, we argue that researchers using LMEMs for confirmatory hypothesis testing should minimally adhere to the standards that have been in place for many decades. Through theoretical arguments and Monte Carlo simulation, we show that LMEMs generalize best when they include the maximal random effects structure justified by the design. The generalization performance of LMEMs including data-driven random effects structures strongly depends upon modeling criteria and sample size, yielding reasonable results on moderately-sized samples when conservative criteria are used, but with little or no power advantage over maximal models. Finally, random-intercepts-only LMEMs used on within-subjects and/or within-items data from populations where subjects and/or items vary in their sensitivity to experimental manipulations always generalize worse than separate F1 and F2 tests, and in many cases, even worse than F1 alone. Maximal LMEMs should be the ‘gold standard’ for confirmatory hypothesis testing in psycholinguistics and beyond.},
}
@article{BatesEtAl2015,
  author = {Douglas Bates and Martin Mächler and Ben Bolker and Steve Walker},
  title = {Fitting Linear Mixed-Effects Models Using lme4},
  journal = {Journal of Statistical Software},
  volume = {67},
  number = {1},
  year = {2015},
  keywords = {sparse matrix methods; linear mixed models; penalized least squares; Cholesky decomposition},
  abstract = {Maximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed- and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.},
  issn = {1548-7660},
  pages = {1--48},
}
@book{Bonferroni1936,
  title = {Teoria statistica delle classi e calcolo delle probabilita},
  author = {Bonferroni, Carlo E},
  year = {1936},
  publisher = {Pubblicazioni del R Istituto Superiore di Scienze Economiche e Commerciali di Firenze},
  number = {8},
  pages = {3--62},
}
@inbook{CliftonEtAl2007,
  title = {Eye Movements in Reading Words and Sentences},
  publisher = {Elsevier Science Ltd.},
  year = {2007},
  pages = {341--374},
  chapter = 15,
  editor = {Roger Van Gompel},
  author = {Charles Clifton and Adrian Staub and Keith Rayner},
  address = {Amsterdam, Netherlands},
  booktitle = {Eye Movements: {A} Window on Mind and Brain},
  abstract = {Word recognition processes seem to be reflected quite straightforwardly in the eyemovement record. In contrast, eye movements seem to reflect sentence comprehension processesin a more varied fashion. We briefly review the major word identification factors that affect eyemovements and describe the role these eye movement phenomena have played in developingtheories of eye movements in reading. We tabulate and summarize one hundred reports of howsyntactic, semantic, pragmatic, and world-knowledge factors affect eye movements duringreading in an initial attempt to identify order in how different types of challenges tocomprehension are reflected in eye movements.},
  keywords = {B_scanpathsimilarity, eyetracking, lexicalretrieval, sentenceprocessing, survey},
}
@article{EngbertEtAl2005,
  author = {Ralf Engbert and Antje Nuthmann and Eike M. Richter and Reinhold Kliegl},
  title = {{SWIFT:  A} Dynamical Model of Saccade Generation During Reading},
  journal = {Psychological Review},
  year = {2005},
  volume = {112},
  pages = {777--813},
  number = {4},
  abstract = {Mathematical models have become an important tool for understanding the control of eye movementsduring reading. Main goals of the development of the SWIFT model (R. Engbert, A. Longtin, & R.Kliegl, 2002) were to investigate the possibility of spatially distributed processing and to implement ageneral mechanism for all types of eye movements observed in reading experiments. The authors presentan advanced version of SWIFT that integrates properties of the oculomotor system and effects of wordrecognition to explain many of the experimental phenomena faced in reading research. They propose newprocedures for the estimation of model parameters and for the test of the model's performance.  They alsopresent a mathematical analysis of the dynamics of the SWIFT model. Finally, within this framework,they present an analysis of the transition from parallel to serial processing.},
  keywords = {B_scanpathsimilarity, article, cognitivemodeling, eyemovements, oculomotorcontrol},
}
@article{GelmanCarlin2014,
  author = {Gelman, Andrew and Carlin, John},
  title = {Beyond Power Calculations: {Assessing} Type {S} (Sign) and Type {M} (Magnitude) Errors},
  journal = {Perspectives on Psychological Science},
  year = {2014},
  month = {11},
  day = {01},
  volume = {9},
  number = {6},
  pages = {641--651},
  abstract = {Statistical power analysis provides the conventional approach to assess error rates when designing a research study. However, power analysis is flawed in that a narrow emphasis on statistical significance is placed as the primary focus of study design. In noisy, small-sample settings, statistically significant results can often be misleading. To help researchers address this problem in the context of their own studies, we recommend design calculations in which (a) the probability of an estimate being in the wrong direction (Type S [sign] error) and (b) the factor by which the magnitude of an effect might be overestimated (Type M [magnitude] error or exaggeration ratio) are estimated. We illustrate with examples from recent published research and discuss the largest challenge in a design calculation: coming up with reasonable estimates of plausible effect sizes based on external information. },
}
@unpublished{GelmanLokenMS,
  author = {Andrew Gelman and Eric Loken},
  title = {The garden of forking paths: {Why} multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time},
  year = {2013},
  abstract = {Researcher degrees of freedom can lead to a multiple comparisons problem, even in settings where researchers perform only a single analysis on their data. The problem is there can be a large number of potential comparisons when the details of data analysis are highly contingent on data, without the researcher having to perform any conscious procedure of fishing or examining multiple p-values. We discuss in the context of several examples of published papers where data-analysis decisions were theoretically-motivated based on previous literature, but where the details of data selection and analysis were not pre-specified and, as a result, were contingent on data.},
}
@article{Holm1979,
  title = {A Simple Sequentially Rejective Multiple Test Procedure},
  author = {Sture Holm},
  journal = {Scandinavian Journal of Statistics},
  number = {2},
  pages = {65--70},
  publisher = {Board of the Foundation of the Scandinavian Journal of Statistics, Wiley},
  volume = {6},
  year = {1979},
  abstract = {This paper presents a simple and widely applicable multiple test procedure of the sequentially rejective type, i.e. hypotheses are rejected one at a time until no further rejections can be done. It is shown that the test has a prescribed level of significance protection against error of the first kind for any combination of true hypotheses. The power properties of the test and a number of possible applications are also discussed.},
}
@article{Kruschke2010,
  author = {John K. Kruschke},
  title = {{What to believe: Bayesian methods for data analysis}},
  journal = {Trends in Cognitive Sciences},
  year = {2010},
  volume = {14},
  pages = {293--300},
  number = {7},
  abstract = {Although Bayesian models of mind have attracted greatinterest from cognitive scientists, Bayesian methods fordata analysis have not.  This article reviews several advantages of Bayesian data analysis over traditional null-hypothesis significance testing. Bayesian methods provide tremendous flexibility for data analytic models and yieldrich information about parameters that can be usedcumulatively across progressive experiments. BecauseBayesian statistical methods can be applied to any data,regardless of the type of cognitive model (Bayesian orotherwise) that motivated the data collection, Bayesianmethods for data analysis will continue to be appropriateeven if Bayesian models of mind lose their appeal.},
  issn = {1364--6613},
  keywords = {bayes, dataanalysis, statistics},
  publisher = {Elsevier},
}
@article{McElreathSmaldino2015,
  author = {McElreath, Richard AND Smaldino, Paul E.},
  journal = {PLoS ONE},
  publisher = {Public Library of Science},
  title = {Replication, Communication, and the Population Dynamics of Scientific Discovery},
  year = {2015},
  month = {08},
  volume = {10},
  number = {8},
  pages = {e0136088},
  abstract = {Many published research results are false (Ioannidis, 2005), and controversy continues over the roles of replication and publication policy in improving the reliability of research. Addressing these problems is frustrated by the lack of a formal framework that jointly represents hypothesis formation, replication, publication bias, and variation in research quality. We develop a mathematical model of scientific discovery that combines all of these elements. This model provides both a dynamic model of research as well as a formal framework for reasoning about the normative structure of science. We show that replication may serve as a ratchet that gradually separates true hypotheses from false, but the same factors that make initial findings unreliable also make replications unreliable. The most important factors in improving the reliability of research are the rate of false positives and the base rate of true hypotheses, and we offer suggestions for addressing each. Our results also bring clarity to verbal debates about the communication of research. Surprisingly, publication bias is not always an obstacle, but instead may have positive impacts—suppression of negative novel findings is often beneficial. We also find that communication of negative replications may aid true discovery even when attempts to replicate have diminished power. The model speaks constructively to ongoing debates about the design and conduct of science, focusing analysis and discussion on precise, internally consistent models, as well as highlighting the importance of population dynamics.},
}

@article{OpenScienceCollaboration2015,
  author = {{Open Science Collaboration}},
  title = {Estimating the reproducibility of psychological science},
  volume = {349},
  number = {6251},
  year = {2015},
  abstract = {Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
  journal = {Science},
}
@article{Rayner1998,
  author = {Keith Rayner},
  title = {Eye movements in reading and information processing: 20 years of research},
  journal = {Psychological Bulletin},
  year = {1998},
  volume = {124},
  pages = {372--422},
  number = {3},
  abstract = {Recent studies of eye movements in reading and other information processing tasks, such as music reading, typing, visual search, and scene perception, are reviewed. The major emphasis of the review is on reading as a specific example of cognitive processing. Basic topics discussed with respect to reading are (a) the characteristics of eye movements, (b) the perceptual span, (c) integration of information across saccades, (d) eye movement control, and (e) individual differences (including dyslexia). Similar topics are discussed with respect to the other tasks examined. The basic theme of the review is that eye movement data reflect moment-to-moment cognitive processes in the various tasks examined. Theoretical and practical considerations concerning the use of eye movement data are also discussed.},
  keywords = {eyemovements, reading, review},
}
@article{ReichleEtAl1998,
  author = {Erik D. Reichle and Alexander Pollatsek and D.L. Fisher and Keith Rayner},
  title = {Toward a Model of Eye Movement Control in Reading.},
  journal = {Psychological Review},
  year = {1998},
  volume = {105},
  pages = {125--157},
  keywords = {eyemovements, model, oculomotorcontrol},
  abstract = {The authors present several versions of a general model, titled the E-Z Reader model, of eye movement control in reading. The major goal of the modeling is to relate cognitive processing (specifically aspects of lexical access) to eye movements in reading. The earliest and simplest versions of the model (E-Z Readers 1 and 2) merely attempt to explain the total time spent on a word before moving forward (the gaze duration) and the probability of fixating a word; later versions (E-Z Readers 3–5) also attempt to explain the durations of individual fixations on individual words and the number of fixations on individual words. The final version (E-Z Reader 5) appears to be psychologically plausible and gives a good account of many phenomena in reading. It is also a good tool for analyzing eye movement data in reading. Limitations of the model and directions for future research are discussed.},
}
@article{SimmonsEtAl2011,
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  title = {False-Positive Psychology: {Undisclosed} Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  year = {2011},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  journal = {Psychological Science},
}
@article{VasishthNicenboim2016,
  title = {Statistical methods for linguistic research: {Foundational} Ideas -- {Part I}},
  author = {Vasishth, Shravan and Nicenboim, Bruno},
  archivePrefix = {arXiv},
  year = {2016},
  primaryClass = {stat-ap},
  keywords = {statistics, methods, tutorial},
	note = {Manuscript published on arXiv 1601.01126},
  abstract = {We present the fundamental ideas underlying statistical hypothesis testing using the frequentist framework. We begin with a simple example that builds up the one-sample t-test from the beginning, explaining important concepts such as the sampling distribution of the sample mean, and the iid assumption. Then we examine the p-value in detail, and discuss several important misconceptions about what a p-value does and does not tell us. This leads to a discussion of Type I, II error and power, and Type S and M error. An important conclusion from this discussion is that one should aim to carry out appropriately powered studies. Next, we discuss two common issues we have encountered in psycholinguistics and linguistics: running experiments until significance is reached, and the "garden-of-forking-paths" problem discussed by Gelman and others, whereby the researcher attempts to find statistical significance by analyzing the data in different ways. The best way to use frequentist methods is to run appropriately powered studies, check model assumptions, clearly separate exploratory data analysis from confirmatory hypothesis testing, and always attempt to replicate results.},
}


@article{VasishthEtAl2013,
  author = {Vasishth, Shravan and von der Malsburg, Titus and Engelmann, Felix},
  title = {What eye movements can tell us about sentence comprehension},
  journal = {Wiley Interdisciplinary Reviews: Cognitive Science},
  volume = {4},
  number = {2},
  publisher = {John Wiley & Sons, Inc.},
  issn = {1939-5086},
  pages = {125--134},
  year = {2013},
  keywords = {eyemovements, method, parsing, scanpaths, corpus},
  abstract = {Eye movement data have proven to be very useful for investigating human sentence processing. Eyetracking research has addressed a wide range of questions, such as recovery mechanisms following garden-pathing, the timing of processes driving comprehension, the role of anticipation and expectation in parsing, the role of semantic, pragmatic, and prosodic information, and so on. However, there are some limitations regarding the inferences that can be made on the basis of eye movements. One relates to the nontrivial interaction between parsing and the eye movement control system which complicates the interpretation of eye movement data. Detailed computational models that integrate parsing with eye movement control theories have the potential to unpack the complexity of eye movement data and can therefore aid in the interpretation of eye movements. Another limitation is the difficulty of capturing spatiotemporal patterns in eye movements using the traditional word-based eyetracking measures. Recent research has demonstrated the relevance of these patterns and has shown how they can be analyzed. In this review, we focus on reading, and present examples demonstrating how eye movement data reveal what events unfold when the parser runs into difficulty, and how the parsing system interacts with eye movement control. WIREs Cogn Sci 2013, 4:125–134. doi: 10.1002/wcs.1209For further resources related to this article, please visit the WIREs website.},
}
@article{Wagenmakers2007,
  title = {A practical solution to the pervasive problems of p values},
  author = {Wagenmakers, Eric-Jan},
  year = {2007},
  issn = {1069-9384},
  journal = {Psychonomic Bulletin \& Review},
  volume = {14},
  number = {5},
  publisher = {Springer-Verlag},
  pages = {779--804},
  abstract = {In the field of psychology, the practice ofp value null-hypothesis testing is as widespread as ever. Despite this popularity, or perhaps because of it, most psychologists are not aware of the statistical peculiarities of thep value procedure. In particular,p values are based on data that were never observed, and these hypothetical data are themselves influenced by subjective intentions. Moreover,p values do not quantify statistical evidence. This article reviews thesep value problems and illustrates each problem with concrete examples. The three problems are familiar to statisticians but may be new to psychologists. A practical solution to thesep value problems is to adopt a model selection perspective and use the Bayesian information criterion (BIC) for statistical inference (Raftery, 1995). The BIC provides an approximation to a Bayesian hypothesis test, does not require the specification of priors, and can be easily calculated from SPSS output.},
}
@incollection{YooEtAl2003,
  title = {{SLURM: Simple} Linux Utility for Resource Management},
  author = {Yoo, Andy B. and Jette, Morris A. and Grondona, Mark},
  year = {2003},
  isbn = {978-3-540-20405-3},
  booktitle = {Job Scheduling Strategies for Parallel Processing},
  volume = {2862},
  series = {Lecture Notes in Computer Science},
  editor = {Feitelson, Dror and Rudolph, Larry and Schwiegelshohn, Uwe},
  publisher = {Springer Berlin Heidelberg},
  pages = {44--60},
  abstract = {A new cluster resource management system called Simple Linux Utility Resource Management (SLURM) is described in this paper. SLURM, initially developed for large Linux clusters at the Lawrence Livermore National Laboratory (LLNL), is a simple cluster manager that can scale to thousands of processors. SLURM is designed to be flexible and fault-tolerant and can be ported to other clusters of different size and architecture with minimal effort. We are certain that SLURM will benefit both users and system architects by providing them with a simple, robust, and highly scalable parallel job execution environment for their cluster system.},
}
@article{EngelmannEtAl2013,
  author = {Engelmann, Felix and Vasishth, Shravan and Engbert, Ralf and Kliegl, Reinhold},
  title = {A Framework for Modeling the Interaction of Syntactic Processing and Eye Movement Control},
  journal = {Topics in Cognitive Science},
  issn = {1756-8765},
  pubstate = {inpress},
  keywords = {Sentence comprehension, Eye movements, Reading, Parsing difficulty, Working memory, Surprisal, Computational modeling},
  year = {2013},
  abstract = {We explore the interaction between oculomotor control and language comprehension on the sentence level using two well-tested computational accounts of parsing difficulty. Previous work (Boston, Hale, Vasishth, & Kliegl, 2011) has shown that surprisal (Hale, 2001; Levy, 2008) and cue-based memory retrieval (Lewis & Vasishth, 2005) are significant and complementary predictors of reading time in an eyetracking corpus. It remains an open question how the sentence processor interacts with oculomotor control. Using a simple linking hypothesis proposed in Reichle, Warren, and McConnell (2009), we integrated both measures with the eye movement model EMMA (Salvucci, 2001) inside the cognitive architecture ACT-R (Anderson et al., 2004). We built a reading model that could initiate short “Time Out regressions” (Mitchell, Shen, Green, & Hodgson, 2008) that compensate for slow postlexical processing. This simple interaction enabled the model to predict the re-reading of words based on parsing difficulty. The model was evaluated in different configurations on the prediction of frequency effects on the Potsdam Sentence Corpus. The extension of EMMA with postlexical processing improved its predictions and reproduced re-reading rates and durations with a reasonable fit to the data. This demonstration, based on simple and independently motivated assumptions, serves as a foundational step toward a precise investigation of the interaction between high-level language processing and eye movement control.},
}

@article{RisseKliegl2011,
  author = {Risse, Sarah and Kliegl, Reinhold},
  ISSN = {0882-7974},
  Journal = {Psychology and Aging},
  Keywords = {age differences, compensation strategies, parafoveal-on-foveal effect, perceptual span, preview benefit, reading, Age Differences, Foveal Vision, Reading, Eye Fixation},
  Number = {2},
  Pages = {451--460},
  Title = {Adult age differences in the perceptual span during reading},
  Volume = {26},
  Year = {2011},
  abstract = {Following up on research suggesting an age-related reduction in the rightward extent of the perceptual span during reading (Rayner, Castelhano, & Yang, 2009), we compared old and young adults in an N + 2-boundary paradigm in which a nonword preview of word N + 2 or word N + 2 itself is replaced by the target word once the eyes cross an invisible boundary located after word N. The intermediate word N + 1 was always three letters long. Gaze durations on word N + 2 were significantly shorter for identical than nonword N + 2 preview both for young and for old adults, with no significant difference in this preview benefit. Young adults, however, did modulate their gaze duration on word N more strongly than old adults in response to the difficulty of the parafoveal word N + 1. Taken together, the results suggest a dissociation of preview benefit and parafoveal-on-foveal effect. Results are discussed in terms of age-related decline in resilience towards distributed processing while simultaneously preserving the ability to integrate parafoveal information into foveal processing. As such, the present results relate to proposals of regulatory compensation strategies older adults use to secure an overall reading speed very similar to that of young adults.}
}

@inproceedings{VasilevAngele2016Psychonomics,
  author = {Martin Vasilev and Bernhard Angele},
  title = {Parafoveal preview effects from word {N+2} during reading: {A Bayesian} meta-analysis},
  year = {2016},
	pages = 197,
  editor = {Bajo, Maria Teresa},
  booktitle = {Proceedings of the International Meeting of the Psychonomic Society},
  publisher = {Psychonomic Society},
  address = {Granada, Spain},
}

@article{MetznerEtAl2016,
  author = {Metzner, Paul and von der Malsburg, Titus and Vasishth, Shravan and Rösler, Frank},
  title = {The importance of reading naturally: {Evidence} from combined recordings of eye movements and electric brain potentials},
  year = {2016},
  journal = {Cognitive Science},
  pubstate = {inpress},
  keywords = {ERP, eye movements, reading, coregistration, n400, p600},
  abstract = {How important is the ability to freely control eye movements for reading comprehension?  And how does the parser make use of this freedom?  We investigated these questions using coregistration of eye movements and event-related brain potentials (ERPs) while participants read either freely or in a computer-controlled word-by-word format (also known as RSVP).  Word-by-word presentation and natural reading both elicited qualitatively similar ERP effects in response to syntactic and semantic violations (N400 and P600 effects).  Comprehension was better in free reading but only in trials in which the eyes regressed to previous material upon encountering the anomaly.  A more fine-grained ERP analysis revealed that these regressions were strongly associated with the well-known P600 effect.  In trials without regressions, we instead found sustained centro-parietal negativities starting at around 320 ms post-onset, however, these negativities were only found when the violation occurred in sentence-final position.  Taken together, these results suggest that the sentence processing system engages in strategic choices: In response to words that don’t match built-up expectations, it can either explore alternative interpretations (reflected by regressions, P600 effects, and good comprehension) or pursue a "good-enough" processing strategy that tolerates a deficient interpretation (reflected by progressive saccades, sustained negativities, and relatively poor comprehension).},
}
